# ä½¿ç”¨ç¤ºä¾‹

æœ¬æ–‡æ¡£æä¾› ControlNet Inpainting çš„å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ã€‚

## ğŸ“š ç›®å½•

1. [ç¯å¢ƒé…ç½®](#1-ç¯å¢ƒé…ç½®)
2. [æ•°æ®å‡†å¤‡](#2-æ•°æ®å‡†å¤‡)
3. [å¿«é€Ÿæµ‹è¯•](#3-å¿«é€Ÿæµ‹è¯•)
4. [è®­ç»ƒæ¨¡å‹](#4-è®­ç»ƒæ¨¡å‹)
5. [æ¨ç†ä½¿ç”¨](#5-æ¨ç†ä½¿ç”¨)
6. [é«˜çº§ç”¨æ³•](#6-é«˜çº§ç”¨æ³•)

---

## 1. ç¯å¢ƒé…ç½®

### æ­¥éª¤ 1ï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨ condaï¼ˆæ¨èï¼‰
conda create -n controlnet python=3.9
conda activate controlnet

# æˆ–ä½¿ç”¨ venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
```

### æ­¥éª¤ 2ï¼šå®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### æ­¥éª¤ 3ï¼šéªŒè¯å®‰è£…

```bash
python test_installation.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
âœ“ PyTorch         - å·²å®‰è£…
âœ“ Diffusers       - å·²å®‰è£…
âœ“ CUDA å¯ç”¨: True
âœ“ GPU 0: NVIDIA GeForce RTX 3090
ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼
```

---

## 2. æ•°æ®å‡†å¤‡

### æ–¹å¼ Aï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰

```bash
bash quick_start.sh
# æŒ‰ç…§æç¤ºé€‰æ‹©ä¸‹è½½ COCO æ•°æ®é›†
```

### æ–¹å¼ Bï¼šæ‰‹åŠ¨ä¸‹è½½

```bash
# åˆ›å»ºç›®å½•
mkdir -p data/coco
cd data/coco

# ä¸‹è½½å›¾åƒï¼ˆ18GBï¼‰
wget http://images.cocodataset.org/zips/train2017.zip
unzip train2017.zip

# ä¸‹è½½æ ‡æ³¨ï¼ˆ241MBï¼‰
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
unzip annotations_trainval2017.zip

cd ../..
```

### éªŒè¯æ•°æ®é›†

```bash
python data.py
```

è¿™å°†ç”Ÿæˆ `data_sample.png`ï¼Œæ˜¾ç¤ºï¼š
- åŸå§‹å›¾åƒ
- é®æŒ¡åçš„å›¾åƒ
- ç”Ÿæˆçš„ mask

---

## 3. å¿«é€Ÿæµ‹è¯•

### æµ‹è¯•æ•°æ®åŠ è½½å™¨

```bash
python data.py
```

### æµ‹è¯•æ¨¡å‹åˆ›å»º

```bash
python model.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼šrunwayml/stable-diffusion-v1-5
å†»ç»“ UNet æƒé‡
æ¨¡å‹åˆå§‹åŒ–å®Œæˆ
è¾“å…¥å½¢çŠ¶ï¼š
  x_t: torch.Size([2, 4, 64, 64])
  timestep: torch.Size([2])
  control_input: torch.Size([2, 4, 64, 64])
è¾“å‡ºå½¢çŠ¶ï¼š
  noise_pred: torch.Size([2, 4, 64, 64])
æ€»å‚æ•°é‡: 1234.56M
å¯è®­ç»ƒå‚æ•°: 456.78M
```

---

## 4. è®­ç»ƒæ¨¡å‹

### åŸºç¡€è®­ç»ƒï¼ˆå• GPUï¼‰

```bash
python train.py \
  --image_dir data/coco/train2017 \
  --ann_file data/coco/annotations/instances_train2017.json \
  --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \
  --batch_size 4 \
  --num_epochs 100 \
  --learning_rate 1e-5 \
  --output_dir ./outputs \
  --mixed_precision fp16 \
  --save_steps 1000 \
  --log_steps 50
```

### å°è§„æ¨¡æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯ï¼‰

```bash
# ä½¿ç”¨å°æ‰¹æ¬¡å’Œå°‘é‡è½®æ•°
python train.py \
  --image_dir data/coco/train2017 \
  --ann_file data/coco/annotations/instances_train2017.json \
  --batch_size 2 \
  --num_epochs 5 \
  --learning_rate 1e-5 \
  --output_dir ./outputs/test_run \
  --save_steps 100
```

### å¤š GPU è®­ç»ƒ

```bash
# é…ç½® accelerate
accelerate config

# å¯åŠ¨è®­ç»ƒ
accelerate launch train.py \
  --image_dir data/coco/train2017 \
  --ann_file data/coco/annotations/instances_train2017.json \
  --batch_size 8 \
  --num_epochs 100 \
  --learning_rate 1e-5 \
  --output_dir ./outputs
```

### ä» checkpoint æ¢å¤è®­ç»ƒ

```bash
python train.py \
  --image_dir data/coco/train2017 \
  --ann_file data/coco/annotations/instances_train2017.json \
  --resume_from_checkpoint outputs/controlnet_inpainting_*/checkpoint-5000
```

### ç›‘æ§è®­ç»ƒè¿›åº¦

```bash
# åœ¨æ–°ç»ˆç«¯ä¸­å¯åŠ¨ TensorBoard
tensorboard --logdir outputs/

# æµè§ˆå™¨æ‰“å¼€
# http://localhost:6006
```

**è®­ç»ƒè¾“å‡ºç¤ºä¾‹ï¼š**
```
Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29500/29500 [2:15:30<00:00, loss=0.1234, lr=9.8e-06]
âœ“ Checkpoint å·²ä¿å­˜åˆ°: outputs/controlnet_inpainting_20241110_120000/checkpoint-1000
Epoch 1/100 å®Œæˆ
å¹³å‡æŸå¤±: 0.1234
```

---

## 5. æ¨ç†ä½¿ç”¨

### åˆ›å»ºæµ‹è¯• mask

```bash
# ä¸è§„åˆ™ç¬”åˆ· mask
python create_test_mask.py --type irregular --output test_mask.png

# çŸ©å½¢ mask
python create_test_mask.py --type rectangle --output test_mask.png

# åœ†å½¢ mask
python create_test_mask.py --type circle --output test_mask.png

# ä¸­å¿ƒçŸ©å½¢ mask
python create_test_mask.py --type center --output test_mask.png
```

### åŸºç¡€æ¨ç†

```bash
python infer.py \
  --checkpoint_path outputs/controlnet_inpainting_*/final_model/controlnet.pth \
  --input_image test_images/sample.jpg \
  --input_mask test_mask.png \
  --output_dir ./outputs/inference \
  --num_inference_steps 50 \
  --scheduler ddpm
```

### å¿«é€Ÿæ¨ç†ï¼ˆDDIMï¼‰

```bash
# DDIM é‡‡æ ·å™¨é€Ÿåº¦æ›´å¿«ï¼ˆ20 æ­¥ vs 50 æ­¥ï¼‰
python infer.py \
  --checkpoint_path outputs/.../final_model/controlnet.pth \
  --input_image test_images/sample.jpg \
  --input_mask test_mask.png \
  --scheduler ddim \
  --num_inference_steps 20
```

### æ‰¹é‡æ¨ç†

```bash
# åˆ›å»ºæ‰¹é‡æ¨ç†è„šæœ¬
cat > batch_infer.sh << 'EOF'
#!/bin/bash

CHECKPOINT="outputs/controlnet_inpainting_*/final_model/controlnet.pth"
INPUT_DIR="test_images"
OUTPUT_DIR="outputs/batch_inference"

for img in $INPUT_DIR/*.jpg; do
    basename=$(basename "$img" .jpg)
    python infer.py \
        --checkpoint_path $CHECKPOINT \
        --input_image "$img" \
        --input_mask "masks/${basename}_mask.png" \
        --output_dir "$OUTPUT_DIR/$basename" \
        --scheduler ddim \
        --num_inference_steps 20
done
EOF

chmod +x batch_infer.sh
./batch_infer.sh
```

**æ¨ç†è¾“å‡ºç¤ºä¾‹ï¼š**
```
ControlNet Inpainting æ¨ç†
è®¾å¤‡: cuda
Checkpoint: outputs/.../controlnet.pth
é‡‡æ ·å™¨: ddpm
æ¨ç†æ­¥æ•°: 50

æ­£åœ¨åŠ è½½æ¨¡å‹...
âœ“ ControlNet æƒé‡å·²åŠ è½½
æ­£åœ¨åŠ è½½ VAE...
æ­£åœ¨åŠ è½½è¾“å…¥...
å›¾åƒå½¢çŠ¶: torch.Size([1, 3, 512, 512])
Mask å½¢çŠ¶: torch.Size([1, 1, 512, 512])

å¼€å§‹å»å™ªè¿‡ç¨‹...
å»å™ª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:15<00:00]

æ­£åœ¨è§£ç å›¾åƒ...
âœ“ è¡¥å…¨ç»“æœå·²ä¿å­˜åˆ°: outputs/inference/inpainted.png
âœ“ å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: outputs/inference/comparison.png

æ¨ç†å®Œæˆï¼
```

---

## 6. é«˜çº§ç”¨æ³•

### 6.1 è‡ªå®šä¹‰æ•°æ®é›†

åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼š

```python
# custom_dataset.py
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms

class CustomInpaintingDataset(Dataset):
    def __init__(self, image_paths, mask_paths):
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        
        self.transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image = Image.open(self.image_paths[idx]).convert('RGB')
        original_image = self.transform(image)
        
        # åŠ è½½ mask
        mask = Image.open(self.mask_paths[idx]).convert('L')
        mask = self.transform(mask)
        mask = (mask > 0.5).float()
        
        # ç”Ÿæˆ masked_image
        masked_image = original_image * (1 - mask) + mask
        
        return original_image, masked_image, mask
```

ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒï¼š

```python
from torch.utils.data import DataLoader
from custom_dataset import CustomInpaintingDataset

# å‡†å¤‡æ•°æ®è·¯å¾„
image_paths = [...]  # ä½ çš„å›¾åƒè·¯å¾„åˆ—è¡¨
mask_paths = [...]   # ä½ çš„ mask è·¯å¾„åˆ—è¡¨

# åˆ›å»ºæ•°æ®é›†
dataset = CustomInpaintingDataset(image_paths, mask_paths)
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

# ç„¶ååœ¨ train.py ä¸­ä½¿ç”¨è¿™ä¸ª dataloader
```

### 6.2 è°ƒæ•´è®­ç»ƒè¶…å‚æ•°

åˆ›å»ºé…ç½®æ–‡ä»¶ `my_config.yaml`ï¼š

```yaml
training:
  batch_size: 8
  learning_rate: 5.0e-6
  num_epochs: 200
  mixed_precision: "fp16"
  
data:
  image_size: 512
  mask_ratio_range: [0.15, 0.35]
  use_irregular_mask: true
```

### 6.3 å¾®è°ƒé¢„è®­ç»ƒçš„ ControlNet

```bash
# ä»å·²æœ‰çš„ checkpoint ç»§ç»­è®­ç»ƒ
python train.py \
  --image_dir data/custom_dataset \
  --ann_file data/custom_annotations.json \
  --pretrained_controlnet_path outputs/.../checkpoint-10000/controlnet.pth \
  --learning_rate 5e-6 \
  --num_epochs 50
```

### 6.4 è¯„ä¼°æ¨¡å‹è´¨é‡

åˆ›å»ºè¯„ä¼°è„šæœ¬ï¼š

```python
# evaluate.py
import torch
from torch.utils.data import DataLoader
from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio
from tqdm import tqdm

from data import create_dataloader
from model import create_model
from diffusers import AutoencoderKL, DDIMScheduler

# åŠ è½½æ¨¡å‹
model = create_model(checkpoint_path="...")
vae = AutoencoderKL.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="vae")
scheduler = DDIMScheduler.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="scheduler")

# è¯„ä¼°æŒ‡æ ‡
ssim = StructuralSimilarityIndexMeasure()
psnr = PeakSignalNoiseRatio()

# è¯„ä¼°å¾ªç¯
for batch in tqdm(test_dataloader):
    original, masked, mask = batch
    
    # æ¨ç†
    inpainted = inference(model, vae, scheduler, masked, mask)
    
    # è®¡ç®—æŒ‡æ ‡
    ssim_score = ssim(inpainted, original)
    psnr_score = psnr(inpainted, original)
    
print(f"SSIM: {ssim_score:.4f}, PSNR: {psnr_score:.2f}")
```

### 6.5 å¯¼å‡º ONNX æ¨¡å‹

```python
# export_onnx.py
import torch
from model import create_model

model = create_model()
model.load_state_dict(torch.load("controlnet.pth"))
model.eval()

# åˆ›å»ºç¤ºä¾‹è¾“å…¥
dummy_input = (
    torch.randn(1, 4, 64, 64),      # x_t
    torch.tensor([500]),             # timestep
    torch.randn(1, 4, 64, 64),      # control_input
)

# å¯¼å‡º
torch.onnx.export(
    model,
    dummy_input,
    "controlnet_inpainting.onnx",
    input_names=['x_t', 'timestep', 'control_input'],
    output_names=['noise_pred'],
    dynamic_axes={
        'x_t': {0: 'batch'},
        'control_input': {0: 'batch'},
        'noise_pred': {0: 'batch'}
    }
)
```

---

## ğŸ’¡ å®ç”¨æŠ€å·§

### æŠ€å·§ 1ï¼šæ˜¾å­˜ä¼˜åŒ–

```bash
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
python train.py ... --gradient_checkpointing

# å‡å°æ‰¹æ¬¡å¤§å°
python train.py ... --batch_size 1

# ä½¿ç”¨æ··åˆç²¾åº¦
python train.py ... --mixed_precision fp16
```

### æŠ€å·§ 2ï¼šåŠ é€Ÿæ¨ç†

```bash
# ä½¿ç”¨ DDIM é‡‡æ ·å™¨
--scheduler ddim --num_inference_steps 20

# ä½¿ç”¨ xformersï¼ˆéœ€è¦é¢å¤–å®‰è£…ï¼‰
pip install xformers
# åœ¨ä»£ç ä¸­å¯ç”¨
model.enable_xformers_memory_efficient_attention()
```

### æŠ€å·§ 3ï¼šè°ƒè¯•æ¨¡å¼

```bash
# åªåœ¨ä¸€ä¸ª batch ä¸Šè®­ç»ƒï¼ˆå¿«é€ŸéªŒè¯ä»£ç ï¼‰
python train.py ... --batch_size 2 --num_epochs 1 --save_steps 10
```

### æŠ€å·§ 4ï¼šå¯è§†åŒ–è®­ç»ƒæ ·æœ¬

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
if global_step % 500 == 0:
    with torch.no_grad():
        sample_output = model(sample_input, ...)
        save_image(sample_output, f'outputs/samples/step_{global_step}.png')
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### è®­ç»ƒæ€§èƒ½

| GPU | æ‰¹æ¬¡å¤§å° | æ··åˆç²¾åº¦ | é€Ÿåº¦ï¼ˆit/sï¼‰| æ˜¾å­˜ä½¿ç”¨ |
|-----|---------|----------|------------|---------|
| RTX 3090 | 4 | FP16 | ~2.1 | 18GB |
| RTX 4090 | 8 | FP16 | ~4.5 | 20GB |
| A100 | 16 | FP16 | ~8.2 | 38GB |

### æ¨ç†æ€§èƒ½

| é‡‡æ ·å™¨ | æ­¥æ•° | æ—¶é—´ï¼ˆ512Ã—512ï¼‰| è´¨é‡ |
|--------|------|----------------|------|
| DDPM | 50 | ~15s | æœ€ä½³ |
| DDIM | 20 | ~6s | è‰¯å¥½ |
| DDIM | 10 | ~3s | ä¸€èˆ¬ |

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **è®­ç»ƒé˜¶æ®µ**
   - ä½¿ç”¨ FP16 æ··åˆç²¾åº¦
   - å®šæœŸä¿å­˜ checkpoint
   - ç›‘æ§ TensorBoard
   - è®­ç»ƒè‡³å°‘ 50K æ­¥

2. **æ¨ç†é˜¶æ®µ**
   - DDPM ç”¨äºæœ€ä½³è´¨é‡
   - DDIM ç”¨äºå¿«é€Ÿæµ‹è¯•
   - è°ƒæ•´ seed è·å¾—ä¸åŒç»“æœ

3. **æ•°æ®å‡†å¤‡**
   - mask å æ¯” 10-30% æ•ˆæœæœ€å¥½
   - ä½¿ç”¨å¤šæ ·åŒ–çš„ mask å½¢çŠ¶
   - ç¡®ä¿å›¾åƒè´¨é‡è‰¯å¥½

---

## â“ æ•…éšœæ’é™¤

### é—®é¢˜ï¼šè®­ç»ƒæŸå¤±ä¸ä¸‹é™

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥å­¦ä¹ ç‡ï¼ˆå°è¯• 1e-5 åˆ° 1e-4ï¼‰
- ç¡®ä¿æ•°æ®æ­£ç¡®åŠ è½½ï¼ˆè¿è¡Œ `python data.py`ï¼‰
- éªŒè¯æ¨¡å‹æƒé‡æ­£ç¡®åŠ è½½

### é—®é¢˜ï¼šæ¨ç†ç»“æœæ¨¡ç³Š

**è§£å†³æ–¹æ¡ˆï¼š**
- å¢åŠ æ¨ç†æ­¥æ•°ï¼ˆ50-100ï¼‰
- ç¡®ä¿æ¨¡å‹è®­ç»ƒå……åˆ†
- å°è¯•ä¸åŒçš„éšæœºç§å­

### é—®é¢˜ï¼šæ˜¾å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
--batch_size 1 --mixed_precision fp16
```

---

**æ›´å¤šé—®é¢˜ï¼Ÿ** æŸ¥çœ‹ [README.md](README.md) æˆ–æäº¤ Issueã€‚

